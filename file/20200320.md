### [2019暑期实习面经](https://www.nowcoder.com/discuss/189431)

### [1.select poll epoll 区别以及各自的应用场景]()

**select的几大缺点：**

- 每次调用 **select** 函数，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 较多时会很大，同时每次调用 **select** 函数都需要在内核遍历传递进来的所有 fd，这个开销在 fd 较多时也很大；
- 单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为 1024，可以通过修改宏定义然后重新编译内核的方式提升这一限制，这样非常麻烦而且效率低下；
- **select** 函数在每次调用之前都要对传入参数进行重新设定，这样做比较麻烦而且会降低性能。

**poll** 与 **select** 相比具有如下优点：

- **poll** 不要求开发者计算最大文件描述符加 1 的大小；
- 相比于 **select**，**poll** 在处理大数目的文件描述符的时候速度更快；
- **poll** 没有最大连接数的限制，原因是它是基于链表来存储的；
- 在调用 **poll** 函数时，只需要对参数进行一次设置就好了。

select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

**当监测的fd数目较小，且各个fd都比较活跃，建议使用select或者poll**

**当监测的fd数目非常大，成千上万，且单位时间只有其中的一部分fd处于就绪状态，这个时候使用epoll能够明显提升性能，比如ngix web服务器就是使用epoll实现的。**



```c
select
select是最早出现的方式：

它可监控的文件描述符数量最多是1024个，
每次进行调用都要将全部描述符从应用进程缓冲区复制到内核缓冲区中，
返回的结果中不会具体声明哪些描述符已经就绪，还需要进行轮询查找
poll
poll
监控描述符的数量没有了限制，但其他缺点和select一样

epoll
epoll

没有监控数量限制
只需要将描述符从进程缓冲区向内核缓冲区拷贝一次
不需要轮询查询哪些就绪，epoll返回后直接告诉进程哪些描述符已经就绪
epoll仅仅适用于linux OS

1. select 应用场景
select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。

select 可移植性更好，几乎被所有主流平台所支持。

2. poll 应用场景
poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。

3. epoll 应用场景
只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。

需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。

需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内
```

### [2. shared_ptr线程安全](https://blog.csdn.net/Solstice/article/details/8547547)

**因为 shared_ptr 有两个数据成员，读写操作不能原子化**

**如果要从多个线程读写同一个 shared_ptr 对象，那么需要加锁**

**引用计数**指的是，所有管理同一个裸指针（raw pointer）的shared_ptr，都共享一个引用计数器，每当一个shared_ptr被赋值（或拷贝构造）给其它shared_ptr时，这个共享的引用计数器就加1，当一个shared_ptr析构或者被用于管理其它裸指针时，这个引用计数器就减1，如果此时发现引用计数器为0，那么说明它是管理这个指针的最后一个shared_ptr了，于是我们释放指针指向的资源

在底层实现中，这个引用计数器保存在某个内部类型里（这个类型中还包含了deleter，它控制了指针的释放策略，默认情况下就是普通的delete操作），而这个内部类型对象在shared_ptr第一次构造时以指针的形式保存在shared_ptr中。shared_ptr重载了赋值运算符，在赋值和拷贝构造另一个shared_ptr时，这个指针被另一个shared_ptr共享。在引用计数归零时，这个内部类型指针与shared_ptr管理的资源一起被释放。此外，为了保证线程安全性，引用计数器的加1，减1操作都是原子操作，它保证shared_ptr由多个线程共享时不会爆掉。

用shared_ptr，不用new
使用weak_ptr来打破循环引用
用make_shared来生成shared_ptr
用enable_shared_from_this来使一个类能获取自身的shared_ptr

### 3.STL 迭代器

1. 对于序列式容器(如vector,deque)，序列式容器就是数组式容器，删除当前的iterator会使后面所有元素的iterator都失效。这是因为vetor,deque使用了连续分配的内存，删除一个元素导致后面所有的元素会向前移动一个位置。所以不能使用erase(iter++)的方式，还好**erase方法可以返回下一个有效的iterator。**

2. vector是一个顺序容器，在内存中是一块连续的内存，当删除一个元素后，内存中的数据会发生移动，以保证数据的紧凑。所以删除一个数据后，其他数据的地址发生了变化，之前获取的迭代器根据原有的信息就访问不到正确的数据。

3. 对于关联容器(如map, set,multimap,multiset)，删除当前的iterator，仅仅会使当前的iterator失效，只要在erase时，递增当前iterator即可。这是因为map之类的容器，使用了红黑树来实现，插入、删除一个结点不会对其他结点造成影响。erase迭代器只是被删元素的迭代器失效，但是返回值为void，所以要采用erase(iter++)的方式删除迭代器。

map是关联容器，以红黑树或者平衡二叉树组织数据，虽然删除了一个元素，整棵树也会调整，以符合红黑树或者二叉树的规范，但是单个节点在内存中的地址没有变化，变化的是各节点之间的指向关系。

总结：迭代器失效分三种情况考虑，也是非三种数据结构考虑，分别为数组型，链表型，树型数据结构。

**数组型数据结构**：该数据结构的元素是分配在连续的内存中，insert和erase操作，都会使得删除点和插入点之后的元素挪位置，所以，插入点和删除掉之后的迭代器全部失效，也就是说insert(*iter)(或erase(*iter))，然后在iter++，是没有意义的。解决方法：erase(*iter)的返回值是下一个有效迭代器的值。 iter =cont.erase(iter);

**链表型数据结构**：对于list型的数据结构，使用了不连续分配的内存，删除运算使指向删除位置的迭代器失效，但是不会失效其他迭代器.解决办法两种，erase(*iter)会返回下一个有效迭代器的值，或者erase(iter++).

**树形数据结构**： 使用红黑树来存储数据，插入不会使得任何迭代器失效；删除运算使指向删除位置的迭代器失效，但是不会失效其他迭代器.erase迭代器只是被删元素的迭代器失效，但是返回值为void，所以要采用erase(iter++)的方式删除迭代器。

### [4.虚继承的实现原理](https://www.nowcoder.com/questionTerminal/638f798ac0da4a89a0c3e893f62d5376)

作用：为了解决从不同途径继承来的同名的数据成员在内存中有不同的拷贝造成数据不一致问题，将共同基类设置为虚基类。 

  这时从不同的路径继承过来的同名数据成员在内存中就只有一个拷贝，同一个函数名也只有一个映射。这样不仅就解决 了二义性问题，也节省了内存，避免了数据不一致的问题。 

  底层实现原理：底层实现原理与编译器相关，一般通过**虚基类指针**实现，即各对象中只保存一份父类的对象，多继承时**通过虚基类指针引用该公共对象**，从而避免菱形继承中的二义性问题。

### 5. memory order内存序

c++11提供了六种内存序供选择，分别为：

```
typedef enum memory_order {
    memory_order_relaxed,
    memory_order_consume,
    memory_order_acquire,
    memory_order_release,
    memory_order_acq_rel,
    memory_order_seq_cst
} memory_order;
```

之前在场景2中，因为指令的重排导致了意料之外的错误，通过使用原子变量并选择合适内存序，可以解决这个问题。下面先来看看这几种内存序

## memory_order_release/memory_order_acquire

内存序选项用来作为原子量成员函数的参数，memory_order_release用于store操作，memory_order_acquire用于load操作，这里我们把使用了memory_order_release的调用称之为release操作。从逻辑上可以这样理解：release操作可以阻止这个调用之前的读写操作被重排到后面去，而acquire操作则可以保证调用之后的读写操作不会重排到前面来。听起来有种很绕的感觉，还是以一个例子来解释：假设flag为一个 atomic特化的bool 原子量，a为一个int变量，并且有如下时序的操作：

| step |                thread A                |                   thread B                   |
| :--: | :------------------------------------: | :------------------------------------------: |
|  1   |                 a = 1                  |                                              |
|  2   | flag.store(true, memory_order_release) |                                              |
|  3   |                                        | if( true == flag.load(memory_order_acquire)) |
|  4   |                                        |                assert(a == 1)                |

实际上这就是把我们上文场景2中的flag变量换成了原子量，并用其成员函数进行读写。在这种情况下的逻辑顺序上，step1不会跑到step2后面去，step4不会跑到step3前面去。这样一来，实际上我们就已经保证了当读取到flag为true的时候a一定已经被写入为1了，场景2得到了解决。换一种比较严谨的描述方式可以总结为：

- 对于同一个原子量，release操作前的写入，一定对随后acquire操作后的读取可见。

这两种内存序是需要配对使用的，这也是将他们放在一起介绍的原因。还有一点需要注意的是：只有对同一个原子量进行操作才会有上面的保证，比如step3如果是读取了另一个原子量flag2，是不能保证读取到a的值为1的。

## memory_order_release/memory_order_consume

memory_order_release还可以和memory_order_consume搭配使用。memory_order_release操作的作用没有变化，而memory_order_consume用于load操作，我们简称为consume操作，comsume操作防止在其后对原子变量有依赖的操作被重排到前面去。这种情况下：

- 对于同一个原子变量，release操作所依赖的写入，一定对随后consume操作后依赖于该原子变量的操作可见。

这个组合比上一种更宽松，comsume只阻止对这个原子量有依赖的操作重拍到前面去，而非像aquire一样全部阻止。将上面的例子稍加改造来展示这种内存序，假设flag为一个 atomic特化的bool 原子量，a为一个int变量，b、c各为一个bool变量，并且有如下时序的操作：

| step |              thread A               |                    thread B                    |
| :--: | :---------------------------------: | :--------------------------------------------: |
|  1   |              b = true               |                                                |
|  2   |                a = 1                |                                                |
|  3   | flag.store(b, memory_order_release) |                                                |
|  4   |                                     | while (!(c = flag.load(memory_order_consume))) |
|  5   |                                     |                 assert(a == 1)                 |
|  6   |                                     |               assert(c == true)                |
|  7   |                                     |               assert(b == true)                |

step4使得c依赖于flag，当step4线程B读取到flag的值为true的时候，由于flag依赖于b，b在之前的写入是可见的，此时b一定为true，所以step6、step7的断言一定会成功。而且这种依赖关系具有传递性，假如b又依赖与另一个变量d，则d在之前的写入同样对step4之后的操作可见。那么a呢？很遗憾在这种内存序下a并不能得到保证，step5的断言可能会失败。

## memory_order_acq_rel

这个选项看名字就很像release和acquire的结合体，实际上它的确兼具两者的特性。这个操作用于“读取-修改-写回”这一类既有读取又有修改的操作，例如CAS操作。可以将这个操作在内存序中的作用想象为将release操作和acquire操作捆在一起，因此任何读写操作的重拍都不能跨越这个调用。依然以一个例子来说明，flag为一个 atomic特化的bool 原子量，a、c各为一个int变量，b为一个bool变量,并且刚好按如下顺序执行：

| step |                   thread A                   |                           thread B                           |
| :--: | :------------------------------------------: | :----------------------------------------------------------: |
|  1   |                    a = 1                     |                                                              |
|  2   |    flag.store(true, memory_order_release)    |                                                              |
|  3   |                                              |                           b = true                           |
|  4   |                                              |                            c = 2                             |
|  5   |                                              | while (!flag.compare_exchange_weak(b, false, memory_order_acq_rel)) {b = true} |
|  6   |                                              |                        assert(a == 1)                        |
|  7   | if (false == flag.load(memory_order_acquire) |                                                              |
|  8   |                assert(c == 2)                |                                                              |

由于memory_order_acq_rel同时具有memory_order_release与memory_order_acquire的作用，因此step2可以和step5组合成上面提到的release/acquire组合，因此step6的断言一定会成功，而step5又可以和step7组成release/acquire组合，step8的断言同样一定会成功。

## memory_order_seq_cst

这个内存序是各个成员函数的内存序默认选项，如果不选择内存序则默认使用memory_order_seq_cst。这是一个“美好”的选项，如果对原子变量的操作都是使用的memory_order_seq_cst内存序，则多线程行为相当于是这些操作都以一种特定顺序被一个线程执行，在哪个线程观察到的对这些原子量的操作都一样。同时，任何使用该选项的写操作都相当于release操作，任何读操作都相当于acquire操作，任何“读取-修改-写回”这一类的操作都相当于使用memory_order_acq_rel的操作。

## memory_order_relaxed

这个选项如同其名字，比较松散，它仅仅只保证其成员函数操作本身是原子不可分割的，但是对于顺序性不做任何保证。

参考：[C++11的原子量与内存序浅析](https://www.cnblogs.com/FateTHarlaown/p/8919235.html)

​           [C++11 中的内存模型](https://tyzual.com/2019/02/11/mem-order/)

​           [C++11的6种内存序](https://blog.csdn.net/qq_22660775/article/details/88800782)

### 6.lock-free & wait-free

**lock-free**:需要取得锁的线程在有限步骤或时间内内就可以成功（多数线程都会成功，一些可能失败，比wait-free语义稍弱）

**wait-free**:需要取得锁的线程在有限步骤或时间内内就可以成功（任意线程都会成功，语义更加强烈）

**（1）wait-free：不管OS如何调度线程，每个线程始终在做有用的事情。**

**（2）lock-free：不管OS如何调度线程，至少有一个线程在做有用的事情。**

**总结**

（1）无锁化并一定能带来高性能，但一定能保证一件事情在确定的时间内完成；

（2）使用无锁化会带来两个问题：性能和crash；

（3）面对无锁化使用的性能问题：采用规避原则，尽可能多核少共享内存资源，少同时操作一个资源；

（4）面对无锁化crash问题，分析了原因是指令重排序，引入了memory_order，制定相应的模型规定指令的执行先后顺序，将多核指令cacheline。

（5）临界区较大一定上锁，小临界区尽可能用原子指令。

对于lock-free算法，如果任何时刻一个操作该数据结构的线程被挂起，其他线程仍然可以访问数据结构并完成相应工作。

wait-free首先是一个lock-free算法，但是进行了加强，线程在有限步内需要完成相应工作。

Lock-Free: 任意时刻至少一个线程在干活

Wait-Free: 任意时刻所有的线程都在干活

### 7.backlog的作用，编程中应该设置为多大

```CQL
int listen(int sockfd, int backlog)
```

**已完成的连接队列**(`ESTABLISHED`)与**未完成连接队列**(`SYN_RCVD`)之和的上限。

![img](https://upload-images.jianshu.io/upload_images/2184951-feff3fcc300fcfcd.png)

![img](https://img-blog.csdn.net/20140319193520921)

参考:[[backlog参数对TCP连接建立的影响](https://segmentfault.com/a/1190000019252960)](https://segmentfault.com/a/1190000019252960)

​	[浅谈tcp socket的backlog参数](https://www.cnblogs.com/qiumingcheng/p/9492962.html)

### 8.[mysql中innodb和myisam的区别](https://www.nowcoder.com/tutorial/93/8ac75a692a3b4b0a868796b9f008bc2c)，[行锁的实现原理](https://lanjingling.github.io/2015/10/10/mysql-hangsuo/)

1）事务：MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持，提供事务支持已经外部键等高级数据库功能。

2）性能：MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快。

3）行数保存：InnoDB 中不保存表的具体行数，也就是说，执行select count() fromtable时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count()语句包含where条件时，两种表的操作是一样的。

4）索引存储：对于AUTO_INCREMENT类型的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中，可以和其他字段一起建立联合索引。MyISAM支持全文索引（FULLTEXT）、压缩索引，InnoDB不支持。

MyISAM的索引和数据是分开的，并且索引是有压缩的，内存使用率就对应提高了不少。能加载更多索引，而Innodb是索引和数据是紧密捆绑的，没有使用压缩从而会造成Innodb比MyISAM体积庞大不小。

InnoDB存储引擎被完全与MySQL服务器整合，InnoDB存储引擎为在主内存中缓存数据和索引而维持它自己的缓冲池。InnoDB存储它的表＆索引在一个表空间中，表空间可以包含数个文件（或原始磁盘分区）。这与MyISAM表不同，比如在MyISAM表中每个表被存在分离的文件中。InnoDB 表可以是任何尺寸，即使在文件尺寸被限制为2GB的操作系统上。

5）服务器数据备份：InnoDB必须导出SQL来备份，LOAD TABLE FROM MASTER操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性(例如外键)的表不适用。

MyISAM应对错误编码导致的数据恢复速度快。MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。

InnoDB是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。

6）锁的支持：MyISAM只支持表锁。InnoDB支持表锁、行锁 行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。

**InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。** InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！

参考:[MySQL 表锁和行锁机制](https://juejin.im/entry/5a55c7976fb9a01cba42786f)

### 9.redis可以做什么

​	通常局限点来说，Redis也以消息队列的形式存在，作为内嵌的List存在，满足实时的高并发需求。而通常在一个电商类型的数据处理过程之中，有关商品，热销，推荐排序的队列，通常存放在Redis之中，期间也包扩Storm对于Redis列表的读取和更新。

### 10. 内存池概念

当 创建大量消耗小内存的对象时，频繁调用new/malloc会导致大量的内存碎片，致使效率降低。内存池的概念就是预先在内存中申请一定数量的，大小相等 的内存块留作备用，当有新的内存需求时，就先从内存池中分配内存给这个需求，不够了之后再申请新的内存。这样做最显著的优势就是能够减少内存碎片，提升效率。

参考:[STL分配器内存池](https://www.nowcoder.com/tutorial/93/8f140fa03c084299a77459dc4be31c95)

### 11. vector deque

![img](https://img-blog.csdn.net/20150826111941696?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![img](https://img-blog.csdn.net/20150826112748719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![这里写图片描述](https://img-blog.csdn.net/20160223191226316)

### [12.数据结构堆](https://juejin.im/post/59fc75f76fb9a0452206dd15#heading-1)

堆的特性：

- **必须是[完全二叉树](http://www.jianshu.com/p/5c9e773344b4)**
- **用数组实现**
- 任一结点的值是其子树所有结点的最大值或最小值
  - 最大值时，称为“最大堆”，也称大顶堆；
  - 最小值时，称为“最小堆”，也称小顶堆

- heap[1]表示堆顶;

- 如果一个有中间结点是i,那么它的左孩子的下标就是2*i,右孩子的下标就是2*i+1，父亲是2*i

- 如果一个结点i，1 <= i <= heap_size/2  那么它有孩子;  heap_size/2 < i <= heap_size 则结点i为叶子结点

- **pop**

  - 正确的做法是将根结点（第一个元素）与最后一个结点（最后一个元素）进行交换，删除最后一个元素，然后再根结点进行一次向下调整。

- push

  - 向上调整：（调整孩子结点child）再来看向上调整，就简单的多了。找到你想调整的结点child，找出它的父亲结点parent，比较两个的大小，若child < parent，不操作。若child > parent, 则交换两个的值，然后令child的下标 = parent的下标。重复以上操作，直到child的下标 <= 0(说明到达根结点)。

    

![ADT](https://user-gold-cdn.xitu.io/2017/11/3/753e2e61c9773d226fb2eba159d4dca8?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

### 13. 析构函数可以抛出异常吗

**从语法上来说，析构函数可以抛出异常，但从逻辑上和风险控制上，析构函数中不要抛出异常，因为栈展开容易导致资源泄露和程序崩溃，所以别让异常逃离析构函数。**

1）如果析构函数抛出异常，则**异常点之后的程序不会执行**，如果析构函数在异常点之后执行了某些必要的动作比如释放某些资源，则这些动作不会执行，会造成诸如资源泄漏的问题。

2）通常异常发生时，c++的机制会调用已经构造对象的析构函数来释放资源，此时若析构函数本身也抛出异常，则前一个异常尚未处理，又有新的异常，会造成程序崩溃的问题。

### [14.进程虚拟地址空间布局](https://www.cnblogs.com/clover-toeic/p/3754433.html)



![img](https://images0.cnblogs.com/i/569008/201405/270929306664122.jpg)

 用户进程部分分段存储内容如下表所示(按地址递减顺序)：

| **名称** | **存储内容**                              |
| -------- | ----------------------------------------- |
| 栈       | 局部变量、函数参数、返回地址等            |
| 堆       | 动态分配的内存                            |
| BSS段    | 未初始化或初值为0的全局变量和静态局部变量 |
| 数据段   | 已初始化且初值非0的全局变量和静态局部变量 |
| 代码段   | 可执行代码、字符串字面值、只读变量        |

### [15.进程间通信IPC (InterProcess Communication)](https://www.jianshu.com/p/c1015f5ffa74)

### [16.支持并发读写的队列](https://blog.csdn.net/mymodian9612/article/details/53608084)

### 17[【面试题】野指针的成因，危害以及避免方法？](https://www.cnblogs.com/asking/p/9460965.html)

